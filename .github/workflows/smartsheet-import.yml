name: Hourly Smartsheet Export

# Grant workflow permission to write back to repo
permissions:
  contents: write

on:
  schedule:
    # Run at the top of every hour
    - cron: '0 * * * *'
  # Allow manual triggering
  workflow_dispatch:

# Prevent multiple concurrent runs of this workflow
concurrency:
  group: smartsheet-import-${{ github.ref }}
  cancel-in-progress: true

jobs:
  export-data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          # ensure the GITHUB_TOKEN is used for pushing
          persist-credentials: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir smartsheet-python-sdk

      - name: Fetch sheet and write CSV + JSON
        env:
          SMARTSHEET_TOKEN: ${{ secrets.SMARTSHEET_TOKEN }}
        run: |
          python - <<'PYCODE'
          import os, csv, json, smartsheet

          # Prepare output directory
          out_dir = 'data'
          os.makedirs(out_dir, exist_ok=True)

          # Initialize Smartsheet client
          token = os.environ['SMARTSHEET_TOKEN']
          ss = smartsheet.Smartsheet(token)

          # Fetch sheet
          sheet = ss.Sheets.get_sheet(3397443560361860)
          cols = [col.title for col in sheet.columns]

          # Build rows for CSV and JSON
          rows_list = []
          rows_json = []
          for row in sheet.rows:
              cell_map = {cell.column_id: cell.value for cell in row.cells}
              row_vals = [cell_map.get(col.id, '') for col in sheet.columns]
              rows_list.append(row_vals)
              rows_json.append({cols[i]: row_vals[i] for i in range(len(cols))})

          # Write CSV
          csv_path = os.path.join(out_dir, 'opportunities.csv')
          with open(csv_path, 'w', newline='', encoding='utf-8') as f:
              writer = csv.writer(f)
              writer.writerow(cols)
              writer.writerows(rows_list)

          # Write JSON with consistent sorting
          json_path = os.path.join(out_dir, 'opportunities.json')
          # Convert to JSON string with consistent key ordering and indentation
          new_json = json.dumps(rows_json, ensure_ascii=False, indent=2, sort_keys=True)
          
          # Only write if the content has changed
          if os.path.exists(json_path):
              with open(json_path, 'r', encoding='utf-8') as f:
                  current_json = f.read()
              if json.loads(current_json) == json.loads(new_json):
                  print('No changes in data, skipping JSON write')
                  new_json = None
          
          if new_json is not None:
              with open(json_path, 'w', encoding='utf-8') as f:
                  f.write(new_json)
          PYCODE

      - name: Commit & push if changed
        run: |
          git config user.name 'github-actions[bot]'
          git config user.email 'github-actions[bot]@users.noreply.github.com'
          if [ -n "$(git status --porcelain data/)" ]; then
            git add data/
            git commit -m 'chore: hourly refresh of opportunities data'
            git push
          else
            echo 'No changes in data/; skipping commit'
          fi
name: Hourly Smartsheet Export

# Grant workflow permission to write back to repo
permissions:
  contents: write

on:
  schedule:
    # Run at the top of every hour
    - cron: '0 * * * *'
  # Allow manual triggering
  workflow_dispatch:

# Prevent multiple concurrent runs of this workflow
concurrency:
  group: smartsheet-import-${{ github.ref }}
  cancel-in-progress: true

jobs:
  export-data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          # ensure the GITHUB_TOKEN is used for pushing
          persist-credentials: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir smartsheet-python-sdk

      - name: Fetch sheets and write CSV + JSON
        env:
          SMARTSHEET_TOKEN: ${{ secrets.SMARTSHEET_TOKEN }}
        run: |
          python - <<'PYCODE'
          import os, csv, json, smartsheet

          SHEETS = [
              {
                  'id': 3397443560361860,
                  'csv': os.path.join('data', 'opportunities.csv'),
                  'json': os.path.join('data', 'opportunities.json')
              },
              {
                  'id': 4160691104599940,
                  'csv': os.path.join('community-events', 'data', 'events.csv'),
                  'json': os.path.join('community-events', 'data', 'events.json')
              }
          ]

          def ensure_parent(path):
              directory = os.path.dirname(path)
              if directory and not os.path.exists(directory):
                  os.makedirs(directory, exist_ok=True)
                  
          def is_valid_row(row_dict, required_fields):
              return all(bool(row_dict.get(field)) for field in required_fields if field in row_dict)

          def write_csv(path, columns, rows):
              ensure_parent(path)
              with open(path, 'w', newline='', encoding='utf-8') as f:
                  writer = csv.writer(f)
                  writer.writerow(columns)
                  writer.writerows(rows)

          def write_json(path, rows):
              ensure_parent(path)
              write_required = True
              if os.path.exists(path):
                  with open(path, 'r', encoding='utf-8') as f:
                      current_data = json.load(f)
                  if current_data == rows:
                      print(f'No changes in {path}, skipping JSON write')
                      write_required = False
              if write_required:
                  with open(path, 'w', encoding='utf-8') as f:
                      json.dump(rows, f, ensure_ascii=False, indent=2, sort_keys=True)

          # Initialize Smartsheet client
          token = os.environ['SMARTSHEET_TOKEN']
          ss = smartsheet.Smartsheet(token)
          ss.errors_as_exceptions(True)

          for sheet_info in SHEETS:
              print(f"Processing sheet {sheet_info['id']}")
              
              # Pagination logic
              all_rows = []
              page_number = 1
              page_size = 5000 # Maximize page size to reduce API calls
              
              while True:
                  print(f"  Fetching page {page_number}...")
                  response = ss.Sheets.get_sheet(
                      sheet_info['id'], 
                      page=page_number, 
                      page_size=page_size, 
                      include='rowPermalink'
                  )
                  
                  all_rows.extend(response.rows)
                  
                  if len(all_rows) >= response.total_row_count:
                      break
                      
                  page_number += 1

              print(f"  Total rows fetched: {len(all_rows)}")
              
              # Use the first response to get columns definition
              # We can reuse 'response' from the last loop iteration or fetch columns separately if needed.
              # But columns are consistent across pages.
              sheet_columns = response.columns

              cols = [col.title for col in sheet_columns if col.title != 'organizer_email']

              rows_csv = []
              rows_json = []
              skipped_rows = 0
              required_fields = ['title', 'startDate', 'region', 'type', 'category', 'format']
              
              for row in all_rows:
                  cell_map = {cell.column_id: cell.value for cell in row.cells}
                  row_dict = {col.title: cell_map.get(col.id, '') for col in sheet_columns}
                  
                  # Skip row if any required field is empty or None
                  if not is_valid_row(row_dict, required_fields):
                      skipped_rows += 1
                      continue
                      
                  # Only include approved events for community-led events
                  if sheet_info['id'] == 4160691104599940:  # Community events sheet
                      if row_dict.get('approved') != True:
                          skipped_rows += 1
                          continue
                      
                  # Remove organizer_email field (don't export to JSON)
                  if 'organizer_email' in row_dict:
                      del row_dict['organizer_email']
                      
                  # Only include columns that exist in the sheet (excluding organizer_email)
                  row_data = []
                  for col in sheet_columns:
                      if col.title != 'organizer_email':
                          row_data.append(cell_map.get(col.id, ''))
                  rows_csv.append(row_data)
                  rows_json.append(row_dict)
                  
              if skipped_rows > 0:
                  print(f"Skipped {skipped_rows} rows with missing required fields")

              write_csv(sheet_info['csv'], cols, rows_csv)
              write_json(sheet_info['json'], rows_json)
          PYCODE

      - name: Check for data changes
        id: check_changes
        run: |
          git config user.name 'github-actions[bot]'
          git config user.email 'github-actions[bot]@users.noreply.github.com'
          CHANGED=$(git status --porcelain data/ community-events/data/)
          if [ -n "$CHANGED" ]; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo 'No changes in data directories; skipping validation and commit'
          fi

      - name: Setup Node.js for validation
        if: steps.check_changes.outputs.has_changes == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install Node dependencies
        if: steps.check_changes.outputs.has_changes == 'true'
        run: npm install

      - name: Validate generated data
        if: steps.check_changes.outputs.has_changes == 'true'
        run: node scripts/validate_data.js

      - name: Run smoke tests
        if: steps.check_changes.outputs.has_changes == 'true'
        run: node scripts/smoke_test.js

      - name: Commit & push if validation passed
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          git add data/ community-events/data/
          git commit -m 'chore: hourly refresh of data feeds'
          git push